
Import table to HDFS:

1. Import all data from the source by deleting target dir (Delete and load).

sqoop import --connect jdbc:mysql://127.0.0.1/custdb --username root -P -table customer 
-m 3 --split-by custid 
--target-dir sqoop_import 
--delete-target-dir 
--direct;

2. Import all data from the source and append to the target dir (Append with all source data, may have duplicates).

sqoop import --connect jdbc:mysql://127.0.0.1/custdb --username root --password root –table customer 
-m 3 --split-by city 
--append --fetch-size 100;


CDC and SDC

3.Import only (inserted) newly added data from the DB to HDFS and get it appended options used (check column, last value incremental append)
  (Append only with newly added source data, No duplicates loaded in the target).
  CDC (Change Data Capture) and SCD (Slowly Changing Dimension) concepts implemented below.

Incremental append:
sqoop import --connect jdbc:mysql://127.0.0.1/custdb --username root --password root -table customer 
-m 1 --target-dir incrimport
--incremental append 
--check-column custid 
--last-value 6

Incremental Lastmodified:

4.Import only (inserted and updated) newly added or modified data from the DB to HDFS and appended. (check column, last value, incremental lastmodified)
  (Append only with newly added and updated source data, Duplicates/history loaded in the target- Slowly changing dimension type 2).

4.1 Importing all data for the first time(Assume 2018-08-18 is the date from when the customer_lastmodified table is created and loaded with data)

sqoop import --connect jdbc:mysql://127.0.0.1/custdb --username root --password root --table customer_lastmodified 
-m 1 --target-dir incrimportlm 
--incremental lastmodified 
--check-column upddt 
--last-value 2018-10-10

4.2 lets add 1 new row and update an existing row with upddt
  
Again run below import then see only two newly added& updated rows will updated on existing dir in hadoop. 

Note --append should need from 2nd time onwards.(append option we are using to append the data in the targer hdfs, if we use –delete-target-dir 
rather than append it deletes and load, if you don’t use any option the load will fail if the target directory already exists.).
  

sqoop import --connect jdbc:mysql://127.0.0.1/custdb --username root --password root --table customer_lastmodified 
-m 1 --target-dir incrimportlm 
--incremental lastmodified 
--check-column upddt 
--last-value 2018-10-11 
--append

SQOOP merge:

5. Import only (inserted and updated) newly added or modified data from the DB to HDFS and get it merged in the target (insert else update).
(check column, last value incremental lastmodified --merge-key custid)
(insert else update - Slowly Changing Dimension Type 1).

sqoop import --connect jdbc:mysql://127.0.0.1/custdb --username root --password root -table customer_lastmodified 
-m 1 --target-dir incrimportlm 
--incremental lastmodified 
--check-column upddt 
--last-value 2018-10-12 
--merge-key custid